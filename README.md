# ‚ù§Ô∏è Comprehensive Machine Learning Full Pipeline on Heart Disease (UCI Dataset)

## üìå General Description
This project analyzes, predicts, and visualizes **heart disease risks** using **Machine Learning**.  
It covers the full pipeline: **data preprocessing, feature selection, dimensionality reduction (PCA), supervised & unsupervised learning, hyperparameter tuning, and deployment**.  

A **Streamlit UI** is developed for real-time interaction and all work is hosted here on GitHub.

---

## üéØ Objectives
- ‚úÖ Perform **Data Preprocessing & Cleaning** (missing values, encoding, scaling).  
- ‚úÖ Apply **Dimensionality Reduction (PCA)** to retain essential features.  
- ‚úÖ Implement **Feature Selection** (statistical & ML-based methods).  
- ‚úÖ Train **Classification Models**: Logistic Regression, Decision Trees, Random Forest, SVM.  
- ‚úÖ Apply **Unsupervised Learning**: K-Means & Hierarchical Clustering.  
- ‚úÖ Perform **Hyperparameter Tuning** with GridSearchCV & RandomizedSearchCV.  
- ‚úÖ Build a **Streamlit UI** for user interaction.  

---

## üõ† Tools & Technologies
- **Languages:** Python  
- **Libraries:** Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn, (TensorFlow/Keras optional)  
- **Dimensionality Reduction & Feature Selection:** PCA, RFE, Chi-Square Test  
- **Supervised Models:** Logistic Regression, Decision Tree, Random Forest, SVM  
- **Unsupervised Models:** K-Means, Hierarchical Clustering  
- **Model Optimization:** GridSearchCV, RandomizedSearchCV  
- **Deployment:** Streamlit, GitHub  

---


---

## üî¨ Workflow

### 1Ô∏è‚É£ Data Preprocessing
- Handle missing values (imputation/removal)  
- Encode categorical features (One-Hot Encoding)  
- Standardize numerical features (MinMaxScaler/StandardScaler)  
- Exploratory Data Analysis (EDA): histograms, heatmaps, boxplots  

**Deliverable:** Clean dataset ready for modeling ‚úÖ  

---

### 2Ô∏è‚É£ Dimensionality Reduction - PCA
- Apply PCA to reduce dimensionality while retaining variance  
- Determine number of components using **explained variance ratio**  
- Visualize results (scatter plot, cumulative variance plot)  

**Deliverable:** PCA-transformed dataset + variance plots ‚úÖ  

---

### 3Ô∏è‚É£ Feature Selection
- Random Forest / XGBoost feature importance  
- Recursive Feature Elimination (RFE)  
- Chi-Square Test  

**Deliverable:** Reduced dataset with selected features + feature importance plots ‚úÖ  

---

### 4Ô∏è‚É£ Supervised Learning (Classification)
- Train/Test split (80/20)  
- Models: Logistic Regression, Decision Tree, Random Forest, SVM  
- Metrics: Accuracy, Precision, Recall, F1-score, ROC, AUC  

**Deliverable:** Trained classification models + evaluation metrics ‚úÖ  

---

### 5Ô∏è‚É£ Unsupervised Learning (Clustering)
- K-Means (elbow method for optimal K)  
- Hierarchical Clustering (dendrogram analysis)  

**Deliverable:** Clustering results with visualizations ‚úÖ  

---

### 6Ô∏è‚É£ Hyperparameter Tuning
- Apply **GridSearchCV** and **RandomizedSearchCV**  
- Compare optimized vs baseline models  

**Deliverable:** Best performing model ‚úÖ  

---

### 7Ô∏è‚É£ Model Export & Deployment
- Save trained model with **joblib/pickle (.pkl)**  
- Build a **Streamlit UI** for user interaction  

**Deliverable:**  
- `final_model.pkl`  
- Functional Streamlit app ‚úÖ  


